{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning! Advanced Local Set-up Required!\n",
    "\n",
    " We've have had some success by setting up a local environment with python 3.7, `tensorflow==1.15.0`, and appropriate versions of `tensorflow-hub`, etc., and removing the `compat.v1` and `.disable_v2_behavior()` commands. The 'Module Install' cell block lists out many of the libraries/versions needed.\n",
    " \n",
    " Google Colab doesn't allow you to set up earlier versions of python, so that is likely not a plausible route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image GANs for Generating Dogs \n",
    "We've been learning about AI-generated images in our module on Vision, Images, and Art. Many computer-generated images, like deep fake videos, leverage something called Generative Adversarial Netowrks (GANs). In this homework, we explore using existing GANs libraries for image generation!\n",
    "\n",
    "Here you will be using a generative adversarial network trained on [ImageNet](http://www.image-net.org/), which we've discussed in lecture previously. Although there are many non-dog classes in ImageNet, we will be focusing our experiments on dog photos. A high quality GAN is tricky to design well and takes dozens of hours or days to train, so to save ourselves computation time, we are using a pre-trained GAN with [TensorFlow](https://www.tensorflow.org/).\n",
    "\n",
    "This activity is largely to familiarize yourself with some of what GANs are capable of doing, experience existing GAN libraries, and practice picking up some new python modules on your own.\n",
    "\n",
    "# Part 1: Setup (6% effort)\n",
    "Your goal for this setup part is simply to successfully run all the code below. This can be frustratingly difficult to setup in Jupyter notebook, so **use Google Colab** for this assignment. Give yourself some credit if you manage to get the modules up and working!\n",
    "\n",
    "This assignment requires a little funky module installation. Remember: the course discussion forum and Student Help Hours are your friend if you hit installation errors, donâ€™t suffer alone!\n",
    "\n",
    "**HIGHLY** recommend you read the Set-up instructions detailed in the GitHub README for this Notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module Install\n",
    "\n",
    "This code uses TensorFlow version 2+ (forced to be compatible with version 1) and Python 3.8+, along with a few other modules. If you're using Google Colab, you probably don't have to install them. The comments in the code below are one potential way to set-up your conda environment to run the notebook locally, although, hopefully, the Google Colab should be a simpler way of doing this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PYTHON VERSION: 3.8.18 (default, Sep 11 2023, 08:28:20) \n",
      "[Clang 14.0.6 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 15:12:53.128110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TENSORFLOW VERSION: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"\\nPYTHON VERSION:\", sys.version)\n",
    "\n",
    "\"\"\" # Possible commandline set-up for conda + running notebook locally \n",
    "# May not be necessary with the last few lines in this cell!\n",
    "conda create -n assignment5 python==3.7.15\n",
    "conda activate assignment5\n",
    "conda install ipykernel --update-deps --force-reinstall\n",
    "conda install tensorflow-estimator=1.15\n",
    "conda install tqdm\n",
    "conda install -c conda-forge opencv\n",
    "conda install scikit-image\n",
    "conda install requests\n",
    "conda install tensorflow-hub=0.8\n",
    "\"\"\"\n",
    "\n",
    "# Using TensorFlow v2 that's compatible with v1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # reduce error messages\n",
    "tf.disable_v2_behavior()\n",
    "print(\"\\nTENSORFLOW VERSION:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJrTM6hAi0CJ"
   },
   "source": [
    "### Environment Setup \n",
    "\n",
    "Now we'll import what we need from the modules we've installed.\n",
    "\n",
    "_Hint_: If you're getting errors on the cell below, you're missing modules! Take notes of which ones you need to install, close this notebook, and view the Github README for set-up instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import urllib\n",
    "import PIL.Image\n",
    "from scipy.stats import truncnorm\n",
    "from skimage import io, data, transform \n",
    "import requests\n",
    "# Using TensorFlow v 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# End tf old school\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow_hub as hub #pip install tensorflow_hub\n",
    "import scipy.misc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import cv2 # pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigGAN Model Setup\n",
    "\n",
    "For this exercise, we will use a pre-trained model called _BigGAN_ generator available on [TensorFlow Hub](https://tfhub.dev/deepmind/biggan-128/2). For more information about this model, check out the authors' paper [_\"Large Scale GAN Training for High Fidelity Natural Image Synthesis\"_ Brock et al. 2019](https://arxiv.org/abs/1809.11096). Next is the address to download this model from TensorFlow Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OJCIhQPClKJ1",
    "outputId": "be805989-fc8e-41bc-f399-4f5d30c953b0"
   },
   "outputs": [],
   "source": [
    "# this model will output 128 by 128 pixel images.\n",
    "module_path ='https://tfhub.dev/deepmind/biggan-128/2'  \n",
    "print(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code Setup\n",
    "\n",
    "This code below is adapted from the [BigGANs Tutorial](https://colab.research.google.com/drive/1rqDwIddy0eunhhV8yrznG4SNiB5XWFJJ) from [Machine Learning for Artists](https://ml4a.github.io/) by Gene Kogan. Many of the exercises here are inspired from that tutorial, so check it out if you want to have more fun with GANs later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Better code design would dictate placing this code in 'helpers.py' and then importing the file, \n",
    "    but to make our GOOGLE COLAB life easier, I've included the helper code in this cell below.\n",
    "    This makes it so we can run this notebook without uploading *any* external files!\n",
    "    There is no need for you to try and understand what this cell block is doing! \n",
    "    Just run it so you can use the helper.py utilities.\n",
    "'''\n",
    "\"\"\"\n",
    "Helper code from the BigGAN tutorial: https://colab.research.google.com/drive/1rqDwIddy0eunhhV8yrznG4SNiB5XWFJJ\n",
    "\n",
    "Adapted for Human-AI Interaction class.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from io import BytesIO\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import urllib\n",
    "import PIL.Image\n",
    "from scipy.stats import truncnorm\n",
    "from skimage import io, data, transform  # pip install scikit-image\n",
    "import requests\n",
    "# Using TensorFlow v 1\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "# End tf old school\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow_hub as hub\n",
    "import scipy.misc\n",
    "from tqdm import tqdm\n",
    "#from random import random\n",
    "import cv2  # pip install opencv-python\n",
    "\n",
    "class GANSession:\n",
    "    def __init__(self, module_path):\n",
    "        ops.reset_default_graph()\n",
    "        print('Loading BigGAN module from:', module_path)\n",
    "        self.module = hub.Module(module_path) # this line currently throws an error\n",
    "        self.inputs = {k: tf.placeholder(v.dtype, v.get_shape().as_list(), k)\n",
    "                       for k, v in self.module.get_input_info_dict().items()}\n",
    "        self.output = self.module(self.inputs)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print('Inputs:\\n', '\\n'.join('  {}: {}'.format(*kv)\n",
    "                                     for kv in self.inputs.items()))\n",
    "        print(\"\\n\")\n",
    "        print('Output:', self.output)\n",
    "\n",
    "        self.input_z = self.inputs['z']\n",
    "        self.input_y = self.inputs['y']\n",
    "        self.input_trunc = self.inputs['truncation']\n",
    "\n",
    "        self.dim_z = self.input_z.shape.as_list()[1]\n",
    "        self.vocab_size = self.input_y.shape.as_list()[1]\n",
    "\n",
    "        # Create a TensorFlow session and initialize variables\n",
    "        initializer = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(initializer)\n",
    "\n",
    "    def truncated_z_sample(self, batch_size, truncation=1., seed=None):\n",
    "        state = None if seed is None else np.random.RandomState(seed)\n",
    "        values = truncnorm.rvs(-2, 2, size=(batch_size,\n",
    "                                            self.dim_z), random_state=state)\n",
    "        return truncation * values\n",
    "\n",
    "    def one_hot(self, index, vocab_size=None):\n",
    "        if(not vocab_size):\n",
    "            vocab_size = self.vocab_size\n",
    "        index = np.asarray(index)\n",
    "        if len(index.shape) == 0:\n",
    "            index = np.asarray([index])\n",
    "        assert len(index.shape) == 1\n",
    "        num = index.shape[0]\n",
    "        output = np.zeros((num, vocab_size), dtype=np.float32)\n",
    "        output[np.arange(num), index] = 1\n",
    "        return output\n",
    "\n",
    "    def one_hot_if_needed(self, label, vocab_size=None):\n",
    "        if(not vocab_size):\n",
    "            vocab_size = self.vocab_size\n",
    "        label = np.asarray(label)\n",
    "        if len(label.shape) <= 1:\n",
    "            label = self.one_hot(label, vocab_size)\n",
    "        assert len(label.shape) == 2\n",
    "        return label\n",
    "\n",
    "    def sample(self, noise, label, truncation=1., batch_size=8, vocab_size=None):\n",
    "        sess = self.sess\n",
    "        if(not vocab_size):\n",
    "            vocab_size = self.vocab_size\n",
    "        noise = np.asarray(noise)\n",
    "        label = np.asarray(label)\n",
    "        num = noise.shape[0]\n",
    "        if len(label.shape) == 0:\n",
    "            label = np.asarray([label] * num)\n",
    "        if label.shape[0] != num:\n",
    "            raise ValueError('Got # noise samples ({}) != # label samples ({})'\n",
    "                             .format(noise.shape[0], label.shape[0]))\n",
    "        label = self.one_hot_if_needed(label, vocab_size)\n",
    "        ims = []\n",
    "        for batch_start in tqdm(range(0, num, batch_size)):\n",
    "            s = slice(batch_start, min(num, batch_start + batch_size))\n",
    "            feed_dict = {self.input_z: noise[s],\n",
    "                         self.input_y: label[s], self.input_trunc: truncation}\n",
    "            ims.append(sess.run(self.output, feed_dict=feed_dict))\n",
    "        ims = np.concatenate(ims, axis=0)\n",
    "        assert ims.shape[0] == num\n",
    "        ims = np.clip(((ims + 1) / 2.0) * 256, 0, 255)\n",
    "        ims = np.uint8(ims)\n",
    "        return ims\n",
    "\n",
    "    def interpolate(self, A, B, num_interps):\n",
    "        alphas = np.linspace(0, 1, num_interps)\n",
    "        if A.shape != B.shape:\n",
    "            raise ValueError(\n",
    "                'A and B must have the same shape to interpolate.')\n",
    "        return np.array([(1-a)*A + a*B for a in alphas])\n",
    "\n",
    "    def imgrid(self, imarray, cols=5, pad=1):\n",
    "        if imarray.dtype != np.uint8:\n",
    "            raise ValueError('imgrid input imarray must be uint8')\n",
    "        pad = int(pad)\n",
    "        assert pad >= 0\n",
    "        cols = int(cols)\n",
    "        assert cols >= 1\n",
    "        N, H, W, C = imarray.shape\n",
    "        rows = int(np.ceil(N / float(cols)))\n",
    "        batch_pad = rows * cols - N\n",
    "        assert batch_pad >= 0\n",
    "        post_pad = [batch_pad, pad, pad, 0]\n",
    "        pad_arg = [[0, p] for p in post_pad]\n",
    "        imarray = np.pad(imarray, pad_arg, 'constant', constant_values=255)\n",
    "        H += pad\n",
    "        W += pad\n",
    "        grid = (imarray\n",
    "                .reshape(rows, cols, H, W, C)\n",
    "                .transpose(0, 2, 1, 3, 4)\n",
    "                .reshape(rows*H, cols*W, C))\n",
    "        if pad:\n",
    "            grid = grid[:-pad, :-pad]\n",
    "        return grid\n",
    "\n",
    "    def interpolate_and_shape(self, A, B, num_samples, num_interps):\n",
    "        interps = self.interpolate(A, B, num_interps)\n",
    "        return (interps.transpose(1, 0, *range(2, len(interps.shape)))\n",
    "                .reshape(num_samples * num_interps, -1))\n",
    "\n",
    "    def get_interpolated_yz(self, categories_all, num_interps, noise_seed_A, noise_seed_B, truncation):\n",
    "        nt = len(categories_all)\n",
    "        num_samples = 1\n",
    "        z_A, z_B = [self.truncated_z_sample(num_samples, truncation, noise_seed)\n",
    "                    for noise_seed in [noise_seed_A, noise_seed_B]]\n",
    "        y_interps = []\n",
    "        for i in range(nt):\n",
    "            category_A, category_B = categories_all[i], categories_all[(\n",
    "                i+1) % nt]\n",
    "            y_A, y_B = [self.one_hot([category] * num_samples)\n",
    "                        for category in [category_A, category_B]]\n",
    "            y_interp = self.interpolate_and_shape(\n",
    "                np.array(y_A), np.array(y_B), num_samples, num_interps)\n",
    "            y_interps.append(y_interp)\n",
    "        y_interp = np.vstack(y_interps)\n",
    "        z_interp = self.interpolate_and_shape(\n",
    "            z_A, z_B, num_samples, num_interps * nt)\n",
    "\n",
    "        return y_interp, z_interp\n",
    "\n",
    "    def get_transition_yz(self, classes, num_interps, truncation):\n",
    "        noise_seed_A, noise_seed_B = 10, 20   # fix this!\n",
    "        return self.get_interpolated_yz(classes, num_interps, noise_seed_A, noise_seed_B, truncation=truncation)\n",
    "\n",
    "    def get_random_yz(self, num_classes, num_interps, truncation):\n",
    "        random_classes = [int(1000*random()) for i in range(num_classes)]\n",
    "        return self.get_transition_yz(random_classes, num_interps, truncation=truncation)\n",
    "\n",
    "    def get_combination_yz(self, categories, noise_seed, truncation):\n",
    "        z = np.vstack([self.truncated_z_sample(1, truncation, noise_seed)]\n",
    "                      * (len(categories)+1))\n",
    "        y = np.zeros((len(categories)+1, 1000))\n",
    "        for i, c in enumerate(categories):\n",
    "            y[i, c] = 1.0\n",
    "            y[len(categories), c] = 1.0\n",
    "        return y, z\n",
    "\n",
    "    def slerp(self, A, B, num_interps):  # see https://en.wikipedia.org/wiki/Slerp\n",
    "        # each unit step tends to be a 90 degree rotation in high-D space, so this is ~360 degrees\n",
    "        alphas = np.linspace(-1.5, 2.5, num_interps)\n",
    "        omega = np.zeros((A.shape[0], 1))\n",
    "        for i in range(A.shape[0]):\n",
    "            tmp = np.dot(A[i], B[i]) / \\\n",
    "                (np.linalg.norm(A[i])*np.linalg.norm(B[i]))\n",
    "            omega[i] = np.arccos(np.clip(tmp, 0.0, 1.0))+1e-9\n",
    "        return np.array([(np.sin((1-a)*omega)/np.sin(omega))*A + (np.sin(a*omega)/np.sin(omega))*B for a in alphas])\n",
    "\n",
    "    def slerp_and_shape(self, A, B, num_interps):\n",
    "        interps = self.slerp(A, B, num_interps)\n",
    "        return (interps.transpose(1, 0, *range(2, len(interps.shape)))\n",
    "                .reshape(num_interps, *interps.shape[2:]))\n",
    "\n",
    "    def imshow(self, a, format='png', jpeg_fallback=True):\n",
    "        a = np.asarray(a, dtype=np.uint8)\n",
    "        str_file = BytesIO()\n",
    "        PIL.Image.fromarray(a).save(str_file, format)\n",
    "        png_data = str_file.getvalue()\n",
    "        try:\n",
    "            disp = IPython.display.display(IPython.display.Image(png_data))\n",
    "        except IOError:\n",
    "            if jpeg_fallback and format != 'jpeg':\n",
    "                print ('Warning: image was too large to display in format \"{}\"; '\n",
    "                       'trying jpeg instead.').format(format)\n",
    "                return self.imshow(a, format='jpeg')\n",
    "            else:\n",
    "                raise\n",
    "        return disp\n",
    "    \n",
    "# You can safely ignore the tensorflow WARNING outputs\n",
    "gan = GANSession(module_path)\n",
    "print(\"This cell takes Colab 1-5 mins).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCeCg3Sdf8Nv"
   },
   "source": [
    "## Part 2: Experimenting with generating deep fake puppies\n",
    "\n",
    "### Task 2.A (2% effort) Choose a dog breed from `dog_classes.txt`, replacing the \"263...'corgi\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "colab_type": "code",
    "id": "HuCO9tv3IKT2",
    "outputId": "885e2289-5133-4038-bd93-410cc5fcdd01"
   },
   "outputs": [],
   "source": [
    "## Task 2.A: Modify this code!\n",
    "truncation = random.uniform(0.02,1) # min:0.02, max:1\n",
    "noise_seed = random.randint(0,100) # min:0, max:100\n",
    "category = \"263: 'Pembroke, Pembroke Welsh corgi'\" # put dog breed here\n",
    "\n",
    "num_samples = 1 # Number of images to generate with these parameters\n",
    "\n",
    "# returns an ndarray, each item is another ndarray is used to generate the images\n",
    "z = gan.truncated_z_sample(num_samples, truncation, noise_seed) \n",
    "y = int(category.split(':')[0])\n",
    "\n",
    "print('truncation: ',truncation, 'noise seed:',noise_seed)\n",
    "\n",
    "ims = gan.sample(z, y, truncation=truncation) # ims is a numpy array\n",
    "max_columns = 20\n",
    "gan.imshow(gan.imgrid(ims, cols=min(num_samples, max_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.B (7% effort) Initial Explorations\n",
    "Run the above image generator cell several times. Experiment! Find your best looking result and place the code in the cell block below. Find your worst looking result, and place it in the cell block after the best one. Be sure to run the cells again, so the images are generated! \n",
    "\n",
    "Describe what you are seeing: what kinds of visual errors do you think this model is creating? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** _Double click this text to write your answer to the question here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST LOOKING RESULT (code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORST LOOKING RESULT (code):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.C (5% effort) Generate a row of dogs with different noise_seeds\n",
    "The code below generates two images with the same `truncation` value, but with two different `noise_seed`s. Adapt this code so that it generates a row of 11 dog images, where each dog has a different value of the parameter `noise_seed` evenly distributed across the range 0 to 100 inclusive (i.e., 0, 10, 20, ...100).\n",
    "\n",
    "For a Welsh Pembroke Corgi with a fixed `truncation` value of `0.95` this looks like the file included in the [Github repository:_`2c_corgi-noise-seeds.png`](https://github.com/UberHowley/haii-a5/blob/main/2c_corgi-noise-seeds.png) where the leftmost Corgi has a `noise_seed` of `0` and the rightmost has a `noise_seed` of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2.C: Modify this code!\n",
    "truncation = 0.95 # min:0.02, max:1\n",
    "category = \"263: 'Pembroke, Pembroke Welsh corgi'\" # put dog breed here\n",
    "\n",
    "num_samples = 1\n",
    "z_list = []\n",
    "\n",
    "# Grab the first element returned by truncated_z_sample\n",
    "z_item = gan.truncated_z_sample(num_samples, truncation, 0)\n",
    "# Add that first element to our list of np.ndarray\n",
    "z_list.append(z_item[0])\n",
    "# Repeat\n",
    "z_item = gan.truncated_z_sample(num_samples, truncation, 100)\n",
    "z_list.append(z_item[0])\n",
    "\n",
    "z = np.asarray(z_list) # convert our list into an np.ndarray\n",
    "y = int(category.split(':')[0])\n",
    "\n",
    "ims = gan.sample(z, y, truncation=truncation)\n",
    "gan.imshow(gan.imgrid(ims,cols=11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.D (12% effort) Generate a grid of dogs, varying noise_seed and truncation\n",
    "Take the code from 2.C above and copy that below. Now, your target output is a _grid_ of dogs. Just as before in 2.C, each _row_ will be 11 generated dogs in an ordered range by `noise_seed` 0-100. Now the _columns_ of the grid shall be different values of the `truncation` parameter, in an ordered range from 0.02-1.0. Your finished grid should have 11 rows, 11 columns, where the top left dog in the grid has a `noise_seed = 0` and `truncation = 0.02` and the values of those parameters increase top to bottom left to right. The dog at the bottom right will have a `noise_seed = 100` and `truncation = 1.0`.\n",
    "\n",
    "If you're unsure if you've got the right grid, you can see what it should look like for corgis in the [Github repository:_`2d_corgi-dog-grid.png`](https://github.com/UberHowley/haii-a5/blob/main/2d_corgi-dog-grid.png) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2.D: Add new code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.E (12% effort) Hypothesize the relationship between noise_seed and truncation. \n",
    "Given your experimentations above and your grid of dogs, hypothesize:\n",
    " - What might be the relationship between `noise_seed` and what a generated dog looks like?\n",
    " - What might be the relationship between `truncation` and what a generated dog looks like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** _Double click this text to write your answer to the question here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHNXtvuQgKwa"
   },
   "source": [
    "# Part 3: Experimenting with transforming puppies\n",
    "\n",
    "In this section, we will experiment with different forms of interpolation to transform and combine different dog images.\n",
    "\n",
    "### Task 3.A (10% effort) Compare truncation and noise_seed across breeds\n",
    "Modify and run the code below, putting in your favorite `truncation` and `noise_seed` values from your experiments in Part 2. Run the code with a pair of closely similar breeds (similar to the Pembroke and the Cardigan Welsh corgies shown below) and a pair of very different breeds. \n",
    " - What do you hypothesize this code is doing with the two dog image samples? Where is `noise_seed_A`'s impact appearing? What about `noise_seed_B`? What is being `interpolated`?\n",
    " - Hypothesize: Does the performance (visual quality of the output) differ based on qualities of the source images such as similar dog breed or similar image background?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** _Double click this text to write your answer to the question here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dSAyfDfnVugs",
    "outputId": "4937d6a3-47cf-4f0d-902f-951c3b4e9e95"
   },
   "outputs": [],
   "source": [
    "## Task 3.A: Modify this code!\n",
    "num_interps = 10 # min:2, max:1000\n",
    "truncation = 0.46 # min:0.02, max:1\n",
    "noise_seed_A = 2 # min:0, max:100\n",
    "category_A = \"263: 'Pembroke, Pembroke Welsh corgi'\" \n",
    "noise_seed_B = 99 # min:0, max:100\n",
    "category_B = \"264: 'Cardigan, Cardigan Welsh corgi'\" \n",
    "\n",
    "y_interp, z_interp = gan.get_interpolated_yz([int(category_A.split(':')[0]), int(category_B.split(':')[0])], num_interps, noise_seed_A, noise_seed_B, truncation=truncation)\n",
    "imgs = gan.sample(z_interp, y_interp, truncation=truncation)\n",
    "gan.imshow(gan.imgrid(imgs, cols=num_interps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.B (8% effort) Different truncations for different dalmations\n",
    "The code below does a different, but similar operation to the code in Task 3.A.\n",
    "\n",
    "Modify and run the code below, putting in your favorite `truncation` and `noise_seed` values from your experiments in Part 2. Run the code with a pair of closely similar breeds (similar to the Pembroke and the Cardigan Welsh corgies shown below) and a pair of very different breeds. \n",
    " - What do you hypothesize this code is doing with the two dog image samples?\n",
    " - Hypothesize: Does the performance (visual quality of the output) differ based on qualities of the source images such as similar dog breed or similar image background?\n",
    " - Unlike in Task 3.A, both images share the same `noise_seed`. From your observations, what do you think is the effect of having the images share the same parameter value versus having each their own `noise_seed`?\n",
    " - What might be the difference between `interpolated` and `combination`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** _Double click this text to write your answer to the question here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "id": "ypWxYQcURofS",
    "outputId": "3a3e736a-0cab-4ee3-8934-0054b2e97af2"
   },
   "outputs": [],
   "source": [
    "## Task 3.B: Modify this code!\n",
    "truncation = 0.45 # min:0.02, max:1\n",
    "noise_seed = 22 # min:0, max:100\n",
    "categoryA = \"263: 'Pembroke, Pembroke Welsh corgi'\" \n",
    "categoryB = \"264: 'Cardigan, Cardigan Welsh corgi'\" \n",
    "\n",
    "categories = [int(categoryA.split(':')[0]), int(categoryB.split(':')[0])]\n",
    "y, z = gan.get_combination_yz(categories, noise_seed, truncation)\n",
    "imgs = gan.sample(z, y, truncation=truncation)\n",
    "gan.imshow(gan.imgrid(imgs, cols=len(categories)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.C (5% effort) Generate your best looking result. \n",
    "Run the above cell several times. Experiment! Find your best looking result and place the code in the cell block below. Be sure to run the cell again, so the images are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST LOOKING RESULT (code):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.D (5% effort) reallyBigGANs\n",
    "A big advantage of the BigGAN model over other GANs is that it is able to produce much higher resolution images than GANs were previously capable of. To test this out, run the cell below. This will load a version of BigGAN that can generate 512 by 512 pixel images instead of 128 by 128 pixels.\n",
    "\n",
    "Now re-run the code for Task 3.B in some exploratory experiments, and store the code that generates your best looking result in the code cell below. Be sure to. run the cell so that the image is generated. _Note: since this generates a larger image, it will take longer to compute_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new GAN Session using our BIG images\n",
    "module_path_big ='https://tfhub.dev/deepmind/biggan-512/2'  \n",
    "gan = GANSession(module_path_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST LOOKING RESULT (code):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.E (5% effort) BigGAN vs not-so-bigGAN\n",
    "Compare your best results from Task 3.C and 3.D. Hypothesize, what do you think the effect of a higher resolution GAN is on the image quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** _Double click this text to write your answer to the question here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Beyond Dogs\n",
    "\n",
    "In this section, we examine our image dataset more closely.\n",
    "\n",
    "### Task 4.A (1% effort) Compare noise_seed across a new category\n",
    "ImageNet (where, unfortunately, we get our dog breed images) has many other classes, listed in `imagenet_classes-all.txt`. Choose one of these categories, and explore the impact `noise_seed` has on the output (i.e., see Task 2C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some time, let's go back to the smaller image set\n",
    "module_path ='https://tfhub.dev/deepmind/biggan-128/2'  \n",
    "gan = GANSession(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.A: Add new code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.B (1% effort) Compare truncation and noise_seed across categories\n",
    "Grab your code from Task 3A and explore how `noise_seed` and `truncation` impact the transition between two different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.B: Add new code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.C (2% effort) Mad Scientist Time!\n",
    "Convert your code from Task 3B into a method, `generate_combo`, that lets you easily combine two classes into one. Explore! Experiment! What's happening?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.D: Complete code below:\n",
    "\n",
    "def generate_combo(categoryA, categoryB, noise_seed, truncation):\n",
    "    pass # replace this with your code!\n",
    "\n",
    "# Calls to generate_combo below:\n",
    "generate_combo(\"386: 'African elephant, Loxodonta africana'\", \"881: 'upright, upright piano'\", 48, 0.99)  # sample call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.E (9% effort) Imagenet and the Unbearable Weight of Massive Methodological Issues\n",
    "\n",
    "This is not our first time encountering ImageNet and its multitudinous problems! Can you recall the previous times ImageNet was brought up? Answer the following questions below (~3 sentences each):\n",
    " - Read the [Excavating AI](https://excavating.ai/) article and summarize its main points. \n",
    " - Did anything in the article surprise you? What do you disagree with? \n",
    " - How do the problems discussed in the [Excavating AI](https://excavating.ai/) article intersect with the ImageNet classes in our `imagenet_classes-all.txt` file? \n",
    " - Is it _wrong_ for the ImageNet authors to censor their dataset? What ethical principles does ImageNet support or contradict?\n",
    " - Is it okay to use datasets that conflict with our values: privacy, morals, social good, equity, etc.? Connect this argument with at least 2 sources from across the semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER (summary):** _Double click this text to write your answer to the question here._\n",
    "\n",
    "**ANSWER (surprised/disagree/agree):** _Double click this text to write your answer to the question here._\n",
    "\n",
    "**ANSWER (apply to imagenet_class-all.txt):** _Double click this text to write your answer to the question here._\n",
    "\n",
    "**ANSWER (ethical principles):** _Double click this text to write your answer to the question here._\n",
    "\n",
    "**ANSWER (connect with 2+ sources):** _Double click this text to write your answer to the question here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Submission\n",
    "\n",
    "Once you've completed all of the above, you're done with this assignment! As always, clean up your code and ensure your entire python Notebook runs before submitting, Iris must be able to run your notebook on her machine.\n",
    "\n",
    "Once you think everything is set, go to `File > Download > Download as .ipynb` and please change the filename of your notebook to `[yourunixID]_haiiYY[assignmentnumber].ipynb`, e.g., `ikh1_haii17a5.ipynb`, and then submit the file on Glow."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BigGAN Workshop",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
